{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9cb1fe2-426e-4526-9b37-1c87047c26f3",
   "metadata": {},
   "source": [
    "# INDEED JOB SCRAPER\n",
    "#### _________ 2-3 jobs/sec __________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04216448-d7ae-4281-a2b5-0638f349ad46",
   "metadata": {},
   "source": [
    "### Enter key words for your custom job role and your scrapeops api key below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89c79925-5ab5-45a5-a991-52b0472d1100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your job role\n",
    "job_role = 'Data Analyst' # example\n",
    "\n",
    "# Location to search in\n",
    "location = 'London%2C+Greater+London' # example\n",
    "\n",
    "# Radius to scan from given location\n",
    "radius = 100 # in miles\n",
    "\n",
    "# Your scrapeops api key (Create an account at https://scrapeops.io/ to get a free apikey)\n",
    "API_KEY = 'your-api-key'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feaf6e3-fd0a-4c04-ac29-ea48451b8cbd",
   "metadata": {},
   "source": [
    "#### _______________________________\n",
    "#### Install required packages (uncomment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26aa1ce2-1acf-47ec-b488-49aaf2d07ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install beautifulsoup4\n",
    "# !pip install glom\n",
    "# !pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "224f1396-6f90-4a38-aeb2-e20ee81657cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/guts/Documents/Projects/Indeed Jobs Webscraping\n"
     ]
    }
   ],
   "source": [
    "# Import packages for scraping\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "# Import packages for data manipulation\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Check current working directory\n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663975a2-9cbf-43b1-ade1-b4d14fed186f",
   "metadata": {},
   "source": [
    "#### Access Job Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57c5b106-999e-4fe8-b925-18540bb879c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your job portal (works only on indeed uk for now)\n",
    "baseUrl = 'https://uk.indeed.com'\n",
    "\n",
    "# Your search params\n",
    "query = job_role.replace(' ', '+').lower()\n",
    "\n",
    "# Create url generator\n",
    "def get_url(query, location, radius):\n",
    "    return f'{baseUrl}/jobs?q={query}&l={location}&radius={radius}'\n",
    "\n",
    "# Create proxy url generator\n",
    "def get_scrapeops_url(url, API_KEY):\n",
    "    payload = {'api_key': API_KEY, 'url': url}\n",
    "    proxy_url = 'https://proxy.scrapeops.io/v1/?' + urlencode(payload)\n",
    "    return proxy_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cedf13-5f5b-490e-83cd-3246696d2f6d",
   "metadata": {},
   "source": [
    "#### Extract Raw Html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0133d150-2aaf-4645-8bb8-c786307e7168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Response [200]>, 'OK')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get custom url\n",
    "url = get_url(query, location, radius)\n",
    "\n",
    "# Send an indirect get request to indeed uk via scrapeops to avoid bot detection\n",
    "response = requests.get(get_scrapeops_url(url, API_KEY))\n",
    "response, response.reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a9a0e06-424c-41b0-9d5e-20d4b9d61138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the content from the response of get-request\n",
    "htmlContent = response.text\n",
    "\n",
    "# Create a soup object from the html content using the lxml parser\n",
    "soup = BeautifulSoup(htmlContent, 'lxml')\n",
    "\n",
    "# Identify all jobcards in the html table tag with their class name\n",
    "cards = soup.find_all('div', 'job_seen_beacon')\n",
    "\n",
    "# Verify number of job cards being displayed on the page\n",
    "len(cards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e2706-4575-47f9-bc8d-e5df19f39d73",
   "metadata": {},
   "source": [
    "### Retreive all jobs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44d038db-f7f3-4dca-b83a-f526e76a6b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that retreives data from a job card\n",
    "def get_record(card):\n",
    "    # Get job title, job url, company name\n",
    "    aTag = card.h2.a\n",
    "    jobTitle = aTag.span.text\n",
    "    jobUrl = baseUrl + aTag.get('href')\n",
    "\n",
    "    # Initiate optional variables\n",
    "    company = 'NA'\n",
    "    companyRating = 'NA'\n",
    "    jobLocation = 'NA'\n",
    "    salary = 'NA'\n",
    "    jobShift = 'NA'\n",
    "    postDate = 'NA'\n",
    "    myJobState = 'not visited'\n",
    "\n",
    "    # Find all corresponding html elements\n",
    "    companySpan = card.find('span', 'companyName')\n",
    "    ratingSpan = card.find('span', 'ratingNumber')\n",
    "    locationDiv = card.find('div', 'companyLocation')\n",
    "    salaryDiv = card.find('div', 'salary-snippet-container')\n",
    "    jobShiftSvg = card.find('svg', {'aria-label':'Job type'})\n",
    "    dateSpan = card.find('span', 'date')\n",
    "    jobStateSpan = card.find('span', 'myJobsState')\n",
    "\n",
    "    # Update all optional variables that are available\n",
    "    if companySpan: company = companySpan.text.strip()\n",
    "    if ratingSpan: companyRating = ratingSpan.span.text.strip()\n",
    "    if locationDiv: jobLocation = locationDiv.text.strip()\n",
    "    if salaryDiv: salary = salaryDiv.text.strip()\n",
    "    if jobShiftSvg: jobShift = jobShiftSvg.parent.text.strip()\n",
    "    if dateSpan: postDate = dateSpan.contents[-1].strip()\n",
    "    if jobStateSpan: myJobState = jobStateSpan.text.strip()\n",
    "    \n",
    "    # Return an indexed pandas series of all data\n",
    "    return pd.Series({'Job Title': jobTitle,\n",
    "                      'Url': jobUrl,\n",
    "                      'Employer': company,\n",
    "                      'Employer Rating': companyRating,\n",
    "                      'Location': jobLocation,\n",
    "                      'Pay': salary,\n",
    "                      'Shift':jobShift,\n",
    "                      'Posted': postDate,\n",
    "                      'Status': myJobState})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f792bb99-205e-4a27-a738-46181d6e71dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Url</th>\n",
       "      <th>Employer</th>\n",
       "      <th>Employer Rating</th>\n",
       "      <th>Location</th>\n",
       "      <th>Pay</th>\n",
       "      <th>Shift</th>\n",
       "      <th>Posted</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data and Performance Analyst</td>\n",
       "      <td>https://uk.indeed.com/rc/clk?jk=0e35c58e8c578c...</td>\n",
       "      <td>Great Ormond Street Hospital NHS Foundation Trust</td>\n",
       "      <td>4.0</td>\n",
       "      <td>London</td>\n",
       "      <td>£49,036 - £55,049 a year</td>\n",
       "      <td>Permanent</td>\n",
       "      <td>Posted 11 days ago</td>\n",
       "      <td>not visited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Flight Data Analyst</td>\n",
       "      <td>https://uk.indeed.com/rc/clk?jk=315edac9b4075e...</td>\n",
       "      <td>British Airways</td>\n",
       "      <td>3.7</td>\n",
       "      <td>Heathrow</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>Posted 10 days ago</td>\n",
       "      <td>not visited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Graduate Data Analyst</td>\n",
       "      <td>https://uk.indeed.com/rc/clk?jk=f8af466bd089af...</td>\n",
       "      <td>Metropolitan Thames Valley</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Remote in London EC1N</td>\n",
       "      <td>£31,500 - £35,000 a year</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Posted 4 days ago</td>\n",
       "      <td>not visited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>https://uk.indeed.com/rc/clk?jk=7819da148cf5f2...</td>\n",
       "      <td>Maggie's</td>\n",
       "      <td>4.2</td>\n",
       "      <td>London</td>\n",
       "      <td>£38,325 - £50,548 a year</td>\n",
       "      <td>Permanent</td>\n",
       "      <td>Posted 3 days ago</td>\n",
       "      <td>not visited</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Job Title  \\\n",
       "0  Data and Performance Analyst   \n",
       "1           Flight Data Analyst   \n",
       "2         Graduate Data Analyst   \n",
       "3                  Data Analyst   \n",
       "\n",
       "                                                 Url  \\\n",
       "0  https://uk.indeed.com/rc/clk?jk=0e35c58e8c578c...   \n",
       "1  https://uk.indeed.com/rc/clk?jk=315edac9b4075e...   \n",
       "2  https://uk.indeed.com/rc/clk?jk=f8af466bd089af...   \n",
       "3  https://uk.indeed.com/rc/clk?jk=7819da148cf5f2...   \n",
       "\n",
       "                                            Employer Employer Rating  \\\n",
       "0  Great Ormond Street Hospital NHS Foundation Trust             4.0   \n",
       "1                                    British Airways             3.7   \n",
       "2                         Metropolitan Thames Valley             3.5   \n",
       "3                                           Maggie's             4.2   \n",
       "\n",
       "                Location                       Pay      Shift  \\\n",
       "0                 London  £49,036 - £55,049 a year  Permanent   \n",
       "1               Heathrow                        NA         NA   \n",
       "2  Remote in London EC1N  £31,500 - £35,000 a year  Full-time   \n",
       "3                 London  £38,325 - £50,548 a year  Permanent   \n",
       "\n",
       "               Posted       Status  \n",
       "0  Posted 11 days ago  not visited  \n",
       "1  Posted 10 days ago  not visited  \n",
       "2   Posted 4 days ago  not visited  \n",
       "3   Posted 3 days ago  not visited  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pandas data frame with all job records\n",
    "recordsDf = pd.DataFrame(columns=['Job Title', 'Url', 'Employer',\n",
    "                                  'Employer Rating', 'Location',\n",
    "                                  'Pay', 'Shift', 'Posted', 'Status'])\n",
    "# Loop through each job card and add it into the recordsDf\n",
    "for card in cards: recordsDf.loc[recordsDf.shape[0]] = get_record(card)\n",
    "    \n",
    "recordsDf.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c905f36-a462-431d-a117-c6f150999879",
   "metadata": {},
   "source": [
    "#### Repeating the same until last page\n",
    "(This code cell might take a few minutes to finish if there are 100s of total jobs because there are only 15 jobs visible per page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a702f33b-abef-4671-b682-a8ef711fddce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(992, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loop until the last page\n",
    "nextPageUrl = True\n",
    "while nextPageUrl:\n",
    "    nextPageUrl = False # this will break the loop if it remains False\n",
    "    # Find the link to go to next page\n",
    "    nextPageATag = soup.find('a', {'aria-label':\"Next Page\"})\n",
    "\n",
    "    if nextPageATag:\n",
    "        # Update the new next page url and get new response\n",
    "        nextPageUrl = baseUrl + nextPageATag.get('href')\n",
    "        newResponse = requests.get(get_scrapeops_url(nextPageUrl, API_KEY))\n",
    "        # create soup object of new response and retrieve all new cards\n",
    "        soup = BeautifulSoup(newResponse.text, 'html.parser')\n",
    "        newCards = soup.find_all('div', 'job_seen_beacon')\n",
    "        # Store each card in the recordsDf\n",
    "        for card in newCards: recordsDf.loc[recordsDf.shape[0]] = get_record(card)\n",
    "\n",
    "# Check final shape\n",
    "recordsDf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee34285-4ea3-4842-8923-650b3af57e36",
   "metadata": {},
   "source": [
    "#### Write data frame to excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b352ef16-25b7-4bb1-854f-56ecb7c3871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the status column\n",
    "newDf = recordsDf.drop(columns='Status') # columns: 9-1 = 8\n",
    "\n",
    "# create an ExcelWriter object for the output file\n",
    "writer = pd.ExcelWriter(f'{job_role} Jobs.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# write newDf for worksheet with the query name\n",
    "newDf.to_excel(writer, sheet_name=query, index=False)\n",
    "\n",
    "# get the workbook and the worksheet\n",
    "workbook = writer.book\n",
    "worksheet = writer.sheets[query]\n",
    "\n",
    "# set the format for the column labels in the worksheet\n",
    "label_format = workbook.add_format({'text_wrap': True, 'align': 'center'})\n",
    "worksheet.set_row(0, None, label_format)\n",
    "\n",
    "# set column widths for all columns\n",
    "worksheet.set_column('A:A', 35)   # Job Title column\n",
    "worksheet.set_column('B:B', 5)    # Url column\n",
    "worksheet.set_column('C:C', 30)   # Employer column\n",
    "worksheet.set_column('D:D', 15)   # Employer Rating column\n",
    "worksheet.set_column('E:E', 25)   # Location column\n",
    "worksheet.set_column('F:F', 20)   # Pay column\n",
    "worksheet.set_column('G:H', 15)   # Shift and Posted columns\n",
    "\n",
    "# save the Excel file\n",
    "writer.save()\n",
    "# Record end time\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8b0e60-ac89-40d0-9e2b-9fb8bc582eeb",
   "metadata": {},
   "source": [
    "#### Performance Statisics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63bf7bde-a36e-44e3-aac9-5943e34ee934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "992 jobs found! in 7 min 24 sec @ 2.23 jobs/sec\n",
      "Saved as 'Data Analyst Jobs.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Calculate total notebook run time in seconds\n",
    "runtime = end_time - start_time\n",
    "# Calculate net speed\n",
    "speed = newDf.shape[0]/runtime\n",
    "\n",
    "# Rounding off\n",
    "rounded_minutes = round(runtime//60)\n",
    "rounded_seconds = round(runtime%60)\n",
    "rounded_speed = round(speed,2)\n",
    "\n",
    "# Print final runtime and speed\n",
    "print(f'\\n{newDf.shape[0]} jobs found! in {rounded_minutes} min {rounded_seconds} sec @ {rounded_speed} jobs/sec')\n",
    "print(f\"Saved as '{job_role} Jobs.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286b39f0-3682-4362-8225-e1a616502281",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
